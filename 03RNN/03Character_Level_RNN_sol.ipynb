{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-Level LSTM in PyTorch\n",
    "\n",
    "이번 실습을 통해 character-level LSTM 를 구현하자. 특정 text에서 문자를 하나씩 학습한다. 그리고 학습한 것을 바탕으로 새로운 문자를 생성하여 문장을 만든다. Anna Karenina라는 소설을 사용하여 모형을 구현해 보자. **소설을 학습하고 소설과 유사한 문장을 생성하는 실습을 해보자.**\n",
    "\n",
    "아래 그림은 일반적인 character-wise RNN의 구조를 나타낸다.\n",
    "\n",
    "![img](../assets/charseq.jpeg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Data\n",
    "\n",
    "Anna Karenina text file을 다운로드하여 학습에 사용할 수 있도록 전처리를 수행한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open text file and read in data as `text`\n",
    "with open('./data/anna.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "처음 100 개 문자를 확인해 보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "문자를 숫자로 전환하기 위한 **dictionary**를 생성한다. 문자를 숫자로 Encoding하여 모형의 input data로 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the text and map each character to an integer and vice versa\n",
    "\n",
    "# we create two dictionaries:\n",
    "# 1. int2char, which maps integers to characters\n",
    "# 2. char2int, which maps characters to unique integers\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "# encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text], dtype=np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "처음 100개 문자를 확인하여 문자가 숫자로 인코딩 되었음을 확인해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([56, 50, 21, 76, 52, 19, 12, 63, 80, 59, 59, 59, 54, 21, 76, 76, 15,\n",
       "       63,  3, 21, 42, 40, 58, 40, 19, 65, 63, 21, 12, 19, 63, 21, 58, 58,\n",
       "       63, 21, 58, 40, 66, 19, 73, 63, 19, 79, 19, 12, 15, 63,  5, 34, 50,\n",
       "       21, 76, 76, 15, 63,  3, 21, 42, 40, 58, 15, 63, 40, 65, 63,  5, 34,\n",
       "       50, 21, 76, 76, 15, 63, 40, 34, 63, 40, 52, 65, 63, 48, 68, 34, 59,\n",
       "       68, 21, 15, 49, 59, 59, 67, 79, 19, 12, 15, 52, 50, 40, 34],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the data\n",
    "\n",
    "LSTM에서는 input을 **one-hot encoded** 으로 사용한다. 따라서 우리가 인코딩한 문자도 one-hot encoding 방식으로 변환해야 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# check that the function works as expected\n",
    "test_seq = np.array([[3, 5, 1]])\n",
    "one_hot = one_hot_encode(test_seq, 8)\n",
    "\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making training mini-batches\n",
    "\n",
    "\n",
    "모형을 훈련하기 위해 mini-batches 를 생성한다. 생성되는 batches 는 아래와 비슷하게 될 것이다:\n",
    "\n",
    "![img](../assets/sequence_batching@1x.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "이 예에서 encoded characters 를 `arr` 라고 하자. 이를 주어진 `batch_size`로 multiple sequences로 나눈다, 각각의 sequences는 `seq_length` 길이를 가진다.\n",
    "\n",
    "### Creating Batches\n",
    "\n",
    "**1. 먼저 full mini-batches로 나누고 남은 문자는 모두 삭제한다.**\n",
    "\n",
    "각 batch는 $N \\times M$ 의 문자를 가진다.(여기서 $N$ 은 batch size = 하나의 batch에 들어가는 sequence 개수) 그리고 $M$ 은 sequence_length 이다. 그 다음에 batches의 총 갯수인 $K$는 `arr` 의 길이를 하나의 batch에 들어가는 문자의 갯수(=$N \\times M$)로 나누면 된다. `arr`에서 필요한 총 문자의 갯수는 $N * M * K$이 된다.\n",
    "\n",
    "**2. `arr`를 $N$ batches로 나눈다.** \n",
    "\n",
    "`arr.reshape(size)`를 사용한다. `size` tuple값으로 준다. 하나의 batch 당 $N$ sequences가 있다. 따라서 $N$이 첫번째 차원값이 된다. reshape를 하고 나면 $N \\times (M * K)$이 된다.\n",
    "\n",
    "**3. 이 array를 사용하여 mini-batches를 생성한다.**\n",
    "\n",
    "$N \\times (M * K)$ array에 대해서 각각의 batch $N \\times M$ window를 가진다. 윈도우는`seq_length`만큼 이동한다. input array와 target arrays를 생성한다. targets은 단지 inputs 을 one character shift한 것이다. `range` 함수를 사용하는데 이때 interval option을 seq_length로 주면 된다. \n",
    "\n",
    "> **실습 :** 아래에 batches 를 생성하는 function를 작성하자. 쉽지 않은 실습이므로 해답 내용을 참조하여 작성해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    ## TODO: Get the number of batches we can make\n",
    "    n_batches = len(arr) // (batch_size*seq_length) #정수나누기해야한다.\n",
    "    \n",
    "    ## TODO: Keep only enough characters to make full batches\n",
    "    arr = arr[:batch_size*seq_length*n_batches]\n",
    "    \n",
    "    ## TODO: Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    ## TODO: Iterate over the batches using a window of size seq_length\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one ==> y는 target으로 x값을 1만큼 shift해서 생성한다\n",
    "        y = np.zeros_like(x)\n",
    "        try :\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]            \n",
    "        yield x, y\n",
    "        # yield는 generator를 return한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Your Implementation\n",
    "\n",
    "batch가 제대로 생성되는지 테스트 해본다. 일단 batch size 는 8 그리고 50 sequence length로 설정해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 8, 50)\n",
    "x, y = next(batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[56 50 21 76 52 19 12 63 80 59]\n",
      " [65 48 34 63 52 50 21 52 63 21]\n",
      " [19 34 60 63 48 12 63 21 63  3]\n",
      " [65 63 52 50 19 63 61 50 40 19]\n",
      " [63 65 21 68 63 50 19 12 63 52]\n",
      " [61  5 65 65 40 48 34 63 21 34]\n",
      " [63 36 34 34 21 63 50 21 60 63]\n",
      " [77 45 58 48 34 65 66 15 49 63]]\n",
      "\n",
      "y\n",
      " [[50 21 76 52 19 12 63 80 59 59]\n",
      " [48 34 63 52 50 21 52 63 21 52]\n",
      " [34 60 63 48 12 63 21 63  3 48]\n",
      " [63 52 50 19 63 61 50 40 19  3]\n",
      " [65 21 68 63 50 19 12 63 52 19]\n",
      " [ 5 65 65 40 48 34 63 21 34 60]\n",
      " [36 34 34 21 63 50 21 60 63 65]\n",
      " [45 58 48 34 65 66 15 49 63 26]]\n"
     ]
    }
   ],
   "source": [
    "# printing out the first 10 items in a sequence\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Defining the network with PyTorch\n",
    "\n",
    "아래와 같은 network를 구성해 보자.\n",
    "\n",
    "![img](../assets/charRNN.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Structure\n",
    "\n",
    "`__init__` 는 다음과 같이 작성한다:\n",
    "* 필요한 dictionary를 생성한다.\n",
    "* LSTM layer는 input size (characters 갯수), hidden layer size `n_hidden`, layers 갯수 `n_layers`, dropout 확률로 `drop_prob`, 그리고 batch_first = True로 설정한다.\n",
    "* dropout layer 를 설정한다.\n",
    "* fully-connected layer는 input size `n_hidden`와 output size (characters의 갯수)로 생성한다.\n",
    "* 최종적으로 weight를 초기화한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### LSTM Inputs/Outputs\n",
    "\n",
    "기본적으로 [LSTM layer](https://pytorch.org/docs/stable/nn.html#lstm) 은 다음과 같이 작성한다.\n",
    "\n",
    "```python\n",
    "self.lstm = nn.LSTM(input_size, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "```\n",
    "\n",
    "`input_size` 는 characters의 갯수이다. sequential input을 받고 `n_hidden`는 hidden layers 에서의 unit 개수이다. dropout을 설정할 수 있다. 마지막으로 `forward` function에서 LSTM cells을 쌓아 올린다.\n",
    "\n",
    "hidden state 의 초기 상태는 모두 0으로 초기화한다.\n",
    "\n",
    "```python\n",
    "self.init_hidden()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU; consider making n_epochs very small.\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        ## TODO: define the layers of the model\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
    "        r_output, hidden = self.lstm(x, hidden) # lstm 수행하면 수행 결과와 새로운 히든이 출력으로.\n",
    "        \n",
    "        out = self.dropout(r_output) # lstm출력을 dropout하여 \n",
    "        \n",
    "        out = out.contiguous().view(-1, self.n_hidden) # shape 바꾸어서 다음 layer로 stack\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        # return the final output and the hidden state\n",
    "       \n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "  \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to train\n",
    "\n",
    "훈련하기 위한 epochs수와 learning rate,그리고 기타 parameters를 적절히 설정한다.\n",
    "\n",
    "Adam optimizer 와 cross entropy loss 를 사용한다. \n",
    " \n",
    "> * gradient가 지수 함수로 증가하는 경우가 있을 수 있다. 이 문제는 exploding gradient 문제로 잘 알려져 있다. [`clip_grad_norm_`](https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html) 를 사용하여  gradients exploding을 방지한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()    \n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                \n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    \n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating the model\n",
    "\n",
    "Network instance를 생성하고 hyperparameters를 설정한다. 그리고 mini-batches sizes를 설정하고 training한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## TODO: set you model hyperparameters\n",
    "# define and print the net\n",
    "n_hidden= 512\n",
    "n_layers= 2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set your training hyperparameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1... Step: 10... Loss: 3.2369... Val Loss: 3.1670\n",
      "Epoch: 1/1... Step: 20... Loss: 3.1443... Val Loss: 3.1287\n",
      "Epoch: 1/1... Step: 30... Loss: 3.1391... Val Loss: 3.1202\n",
      "Epoch: 1/1... Step: 40... Loss: 3.1170... Val Loss: 3.1184\n",
      "Epoch: 1/1... Step: 50... Loss: 3.1393... Val Loss: 3.1159\n",
      "Epoch: 1/1... Step: 60... Loss: 3.1143... Val Loss: 3.1123\n",
      "Epoch: 1/1... Step: 70... Loss: 3.0998... Val Loss: 3.1068\n",
      "Epoch: 1/1... Step: 80... Loss: 3.1077... Val Loss: 3.0929\n",
      "Epoch: 1/1... Step: 90... Loss: 3.0798... Val Loss: 3.0564\n",
      "Epoch: 1/1... Step: 100... Loss: 3.0022... Val Loss: 2.9684\n",
      "Epoch: 1/1... Step: 110... Loss: 2.8909... Val Loss: 2.8570\n",
      "Epoch: 1/1... Step: 120... Loss: 2.7189... Val Loss: 2.6920\n",
      "Epoch: 1/1... Step: 130... Loss: 2.6426... Val Loss: 2.6005\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 1 # start small if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, \n",
    "      epochs=n_epochs, batch_size=batch_size, \n",
    "      seq_length=seq_length, lr=0.001,\n",
    "      print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "설정이 필요한 hyperparameters 는 다음과 같다..\n",
    "\n",
    "* `n_hidden` - The number of units in the hidden layers.\n",
    "* `n_layers` - Number of hidden LSTM layers to use.\n",
    "\n",
    "* `batch_size` - Number of sequences running through the network in one pass.\n",
    "* `seq_length` - Number of characters in the sequence. 보통 큰 값으로 설정하면 더 긴 내용을 학습할 수 있다.너무 크게 하면 학습이 오래 걸린다. \n",
    "* `lr` - Learning rate for training\n",
    "\n",
    "## Tips and Tricks\n",
    "\n",
    ">### Validation Loss vs. Training Loss을 확인한다.\n",
    ">- 보통 training loss와 validation loss 의 차이가 심하면 **overfitting**된다고 본다. 그런 경우에는 network size를 줄이거나 dropout을 설정한다.\n",
    ">- 만약, training/validation loss가 같으면 **underfitting**되었다고 본다. 그럴 경우에는 layer의 갯수, layer당 unit의 갯수를 증가한다.\n",
    "\n",
    ">### 적절한 hyper parameter를 설정한다.\n",
    "> 중요한 parameters는 `n_hidden`,`n_layers` 2개 이다. 이 parameter는 데이터셋의 크기에 따라 달라진다. 보통의 경우에는 전체 train resource를 고려하여 좀 더 큰 네트워크를 만들어서 train을 진행한다. validation, train loss를 살펴보고 overfitting 되면 dropout 등을 추가적으로 주고 훈련을 진행한다. 이때 validation loss가 최소가 되는 모형을 최종적인 모형으로 저장한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "\n",
    "훈련이 종료되면 최종적인 모형을 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the name, for saving multiple files\n",
    "model_name = 'rnn_1_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Making Predictions\n",
    "\n",
    "모형이 잘 작동하여 문장을 제대로 생성하는지 확인해 본다.\n",
    "\n",
    "### A note on the `predict`  function\n",
    "\n",
    "RNN의 출력 값은 주어진 입력값 다음에 나올 문자를 예측하여 출력한다. 문자의 score값을 출력하므로 이를 확률로 전환해야 한다. 확률로 전환하기 위해 softmax함수를 사용한다.\n",
    "\n",
    "> softmax function를 적용하면 주어진 입력 다음에 나올 문자에 대한 확률값을 출력하게 된다.\n",
    "\n",
    "### Top K sampling\n",
    "\n",
    "다음 나올 문자의 확률이 높은 k개의 문자만이 최종 출력되도록 해야한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    " \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priming and generating text \n",
    "\n",
    "보통 문장의 시작 단어를 prime 값(=초기값)으로 준다. 그러면 이 문자열을 기반으로 하여 문장을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annate hhat,. \n",
      "hed hosing tar hire tisitothes tho tote tared theritid whe sot tare tored seed son tet has san hase sit ote wir hh tit ond astis aft of ofte shete tee nnter thin tor, at tor tee ante hat tar soter tor wote te wot the sosd he thing on te titidg nodd of an tor tating as tire to ne ant an hed he te ho hitte\n",
      "song the sat an tim terire sond tote sond sor and totin tim at he anges sere he ton to hor se son sor ond tire has tetese as he himed af tet thot,, sottas an han ans simos he hind tir an time his ses ate tan sititere asdimetan th and thot or the to the tired whe afise sore hanger as antho hor anse sorese son ante she tir sote siros ath raris an wame thin tart hot tore an som sar on sh sas ha sed te hose tond the ha sos at att ant in tan hhe tanserann hiritit an sir sher som anthe and.\n",
      "\n",
      "An sis thos wot tith not at anthe anthe sang on har an hostin al afing and tor than sor as has an sotin we sere har himt to hot an al af hit to tore se af timethe to hhere\n",
      "\"n\n",
      "alte sitit in tito\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime='Anna', top_k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
    "with open('rnn_20_epoch.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f, map_location=lambda storage, loc: storage)\n",
    "\n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And Levin said to him was that he did\n",
      "not have to be all so that they should be in learned he had not been bought on horrible to him that he had seemed to him, but he had not been, as though\n",
      "the conversation and her face, and stalling her eyes, they cellantly dropped the sight\n",
      "of\n",
      "his wife, he could not come at the bustes of the pleasure, and a lot of to constant.\n",
      "\n",
      "Anna still said, as the chorce had been sent and stopped. The death was breaking up her ears, at the\n",
      "point to the place, all sort of her eyes was so subject and tears and to him, and as though she had not been been to see him.\n",
      "\n",
      "\"I can't go into my subject,\" said the priest. \"It's already that I can't go into that instatt, but I'm not it\n",
      "supper, but I'm not at the sareny?\" he said, with the stair of the priest of a\n",
      "present, and while she went\n",
      "on an expression.\n",
      "\n",
      "As he truets, with still a faces that he\n",
      "with a first special principle to her feeling, and at his had was being\n",
      "begun to still thought\n",
      "that he\n",
      "had\n",
      "come from her. But that was that the candle was to go to his face.\n",
      "\n",
      "\"Yes, if, I dount to meet anything.\n",
      "In the straight of her mother the same all at all these probrows.\n",
      "\n",
      "\n",
      "\n",
      "Chapter 13\n",
      "\n",
      "\n",
      "The mind there was a letter in her side of a parties.\n",
      "\n",
      "\"It's something still to\n",
      "do\n",
      "it. And he went to the sarriving, with the minutes, as they could never have nothing been as it all. This was no mind of marriage, to\n",
      "hear this\n",
      "still more almost anything but her to be tried, but as to see him. A sould of this wish it seems to be so milly and set off in the master. And the position is to suppose it.\"\n",
      "\n",
      "\"What is that\n",
      "the same all that?\"\n",
      "\n",
      "\"No, you were a given me shall be one as yet you arring a meanon,\" and the same same\n",
      "time with which the same a princess would have soul a song of the decide to the man who had been at the person of the people\n",
      "and that it was not\n",
      "to blame at all to the place, and at the\n",
      "same and his bound he had not been thinking\n",
      "when the carriage to\n",
      "begin to hear them along the things to say something in the carriage. She w\n"
     ]
    }
   ],
   "source": [
    "# Sample using a loaded model\n",
    "print(sample(loaded, 2000, top_k=5, prime=\"And Levin said\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
