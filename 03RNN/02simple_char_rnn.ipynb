{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple char-RNN 실습\n",
    "\n",
    "![img](../assets/simple_char_rnn.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding\n",
    "훈련시키고자 하는 character를 수치화하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = [1,0,0,0]\n",
    "e = [0,1,0,0]\n",
    "l = [0,0,1,0]\n",
    "o = [0,0,0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN cell 생성\n",
    "- input_dim = 4 <-- one hot encoding  \n",
    "- hidden_size = 2 <-- output으로 나오는 갯수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = nn.RNN(input_size = 4, hidden_size=2, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([[h]], dtype=torch.float32) # <- 문자 1개 \n",
    "print(inputs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = torch.randn(1,1,2) # init hidden은 랜덤값으로 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.6755,  0.6619]]])\n"
     ]
    }
   ],
   "source": [
    "out, hidden = cell(inputs, hidden)\n",
    "print(out.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Sequence\n",
    "input값을 넣을때 단일 문자만을 넣는게 아니라 여러개의 문자를 입력으로 준다.  \n",
    "즉, 단일한 문자 여러개를 하나의 sequence로 묶어 입력으로 제공한다.  \n",
    "기본적으로 위의 예와 같고 입력에서 단일값이 아니라 문자를 여러개 입력으로 준다.\n",
    "- hidden_size = 2\n",
    "- **seq_len = 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size : torch.Size([1, 5, 4])\n",
      "tensor([[[-0.4777, -0.3481],\n",
      "         [-0.7116, -0.3802],\n",
      "         [-0.6112,  0.5556],\n",
      "         [-0.0536, -0.0232],\n",
      "         [-0.2218,  0.0640]]])\n"
     ]
    }
   ],
   "source": [
    "cell = nn.RNN(input_size = 4, hidden_size=2, batch_first=True)\n",
    "\n",
    "inputs = torch.tensor([[h,e,l,l,o]], dtype=torch.float32) # <- 문자 n개를 sequence로 준다.\n",
    "print(\"input_size :\",inputs.size())\n",
    "\n",
    "hidden = torch.randn(1,1,2) # init hidden은 랜덤값으로 \n",
    "\n",
    "out, hidden = cell(inputs, hidden)\n",
    "print(out.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch size\n",
    "앞의 예제에 더 붙여 한 번에 하나의 단어만 입력하는 것이 아니라 여러개의 단어(= character의 sequence)를 입력하기 위해서 batch_size를 준다.\n",
    "\n",
    "- hidden_size = 2\n",
    "- seq_len = 5\n",
    "- **batch_size = 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size : torch.Size([3, 5, 4])\n",
      "tensor([[[-0.6966,  0.3133],\n",
      "         [ 0.0866,  0.6837],\n",
      "         [-0.7175,  0.7736],\n",
      "         [-0.6753,  0.7137],\n",
      "         [-0.7392,  0.1297]],\n",
      "\n",
      "        [[ 0.1937,  0.5673],\n",
      "         [-0.7728,  0.2524],\n",
      "         [-0.6082,  0.6009],\n",
      "         [-0.6632,  0.6937],\n",
      "         [-0.6700,  0.7050]],\n",
      "\n",
      "        [[-0.6686,  0.6897],\n",
      "         [-0.6691,  0.7037],\n",
      "         [ 0.0029,  0.7544],\n",
      "         [-0.0914,  0.8148],\n",
      "         [-0.7198,  0.7778]]])\n"
     ]
    }
   ],
   "source": [
    "cell = nn.RNN(input_size = 4, hidden_size=2, batch_first=True)\n",
    "\n",
    "inputs = torch.tensor([[h,e,l,l,o],\n",
    "                       [e,o,l,l,l],\n",
    "                       [l,l,e,e,l]], dtype=torch.float32) # <- 문자 n개를 sequence로 준다.\n",
    "print(\"input_size :\",inputs.size())\n",
    "\n",
    "hidden = torch.randn(1,3,2) # init hidden은 랜덤값으로 , batch_size를 맞추라! \n",
    "\n",
    "out, hidden = cell(inputs, hidden)\n",
    "print(out.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chararater-level RNN\n",
    "\n",
    "\n",
    "![img](../assets/simple_char_rnn1.png)\n",
    "\n",
    "hihell을 입력하면 ihello를 생성하는 RNN 모형을 생성하자.  \n",
    "hihello의 unique 값 --> h, i, e, l, o ... 5개이다.  \n",
    "이것을 one hot encoding하면  \n",
    "h --> [1,0,0,0,0]  \n",
    "i --> [0,1,0,0,0]  \n",
    "e --> [0,0,1,0,0]  \n",
    "l --> [0,0,0,1,0]  \n",
    "o --> [0,0,0,0,1]  \n",
    "\n",
    "- input_dim = 5 \n",
    "- output_dim = 5\n",
    "\n",
    "## loss function\n",
    "multi class classification 이므로 cross_entropy를 쓴다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "idx2char = ['h','i','e','l','o'] # 사용할 기본 voca에 해당, unique 문자\n",
    "\n",
    "x_data = [0,1,0,2,3,3] # 입력 hihell --> 위에 생성한 voca의 index값으로 입력\n",
    "\n",
    "x_one_hot = [[[1,0,0,0,0], # h\n",
    "              [0,1,0,0,0], # i\n",
    "              [1,0,0,0,0], # h\n",
    "              [0,0,1,0,0], # e\n",
    "              [0,0,0,1,0], # l\n",
    "              [0,0,0,1,0]  # l\n",
    "              ]]\n",
    "\n",
    "y_data = [1,0,2,3,3,4] # ihello를 출력하게 --> index값으로 출력\n",
    "\n",
    "x_one_hot = []\n",
    "\n",
    "inputs = torch.tensor(x_one_hot, dtype=torch.float32) # input은 반드시 one-hot vector가 되어야\n",
    "labels = torch.tensor(y_data)\n",
    "\n",
    "print(inputs.type(), labels.type())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리 2\n",
    "앞의 예와 동일한데 다만 다른 점은 one_hot_lookup을 만들고 이로 부터 x_one_hot을 만드는 부분이 다르다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2char = ['h','i','e','l','o'] # 사용할 기본 voca에 해당, unique 문자\n",
    "\n",
    "x_data = [0,1,0,2,3,3] # 입력 hihell --> 위에 생성한 vaca의 index값으로 입력\n",
    "\n",
    "\n",
    "# index 값으로 lookup 을 생성한다. \n",
    "one_hot_lookup = [ [1,0,0,0,0], # 0\n",
    "                   [0,1,0,0,0], # 1\n",
    "                   [0,0,1,0,0], # 2\n",
    "                   [0,0,0,1,0], # 3\n",
    "                   [0,0,0,1,0] # 4\n",
    "                 ]\n",
    "\n",
    "y_data = [1,0,2,3,3,4] # ihello를 출력하게 --> index값으로 출력\n",
    "\n",
    "x_one_hot = [one_hot_lookup[x] for x in x_data] # 이 부분이 다르다. loopkup으로 부터 one-hot vector 생성\n",
    "\n",
    "inputs = torch.tensor(x_one_hot, dtype=torch.float32) # input은 반드시 one-hot vector가 되어야\n",
    "labels = torch.tensor(y_data).unsqueeze(1)\n",
    "\n",
    "#print(inputs.size(), labels.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module) :\n",
    "    def __init__(self, input_size, hidden_size,) :\n",
    "        super(Model, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=input_size, \n",
    "                          hidden_size=hidden_size, batch_first=True)\n",
    "    \n",
    "    def forward(self, x, hidden) :\n",
    "        x = x.view(batch_size, sequence_length, input_size)\n",
    "        \n",
    "        # Input: (batch, seq_len, input_size)\n",
    "        # hidden: (num_layers * num_directions, batch, hidden_size)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "        out = out.view(-1, num_classes) \n",
    "        \n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self) :\n",
    "        return torch.zeros(num_layers, batch_size, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모형의 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (rnn): RNN(5, 5, batch_first=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_classes = 5\n",
    "input_size = 5  # one-hot size\n",
    "hidden_size = 5  # output from the RNN. 5 to directly predict one-hot\n",
    "batch_size = 1   # one sequence\n",
    "sequence_length = 1  # One by one\n",
    "num_layers = 1  # one-layer rnn\n",
    "\n",
    "# Instantiate RNN model\n",
    "model = Model(input_size, hidden_size)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set loss and optimizer function\n",
    "# CrossEntropyLoss = LogSoftmax + NLLLoss\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모형의 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted string : llllll, epoch: 1, loss: 9.317\n",
      "predicted string : llllll, epoch: 2, loss: 9.201\n",
      "predicted string : llllll, epoch: 3, loss: 9.088\n",
      "predicted string : lhllll, epoch: 4, loss: 8.977\n",
      "predicted string : lhllll, epoch: 5, loss: 8.868\n",
      "predicted string : lhllll, epoch: 6, loss: 8.760\n",
      "predicted string : lhllll, epoch: 7, loss: 8.653\n",
      "predicted string : lhllll, epoch: 8, loss: 8.548\n",
      "predicted string : lhllll, epoch: 9, loss: 8.445\n",
      "predicted string : lhelll, epoch: 10, loss: 8.343\n",
      "predicted string : lhelll, epoch: 11, loss: 8.242\n",
      "predicted string : lhelll, epoch: 12, loss: 8.143\n",
      "predicted string : lhelll, epoch: 13, loss: 8.045\n",
      "predicted string : lhelll, epoch: 14, loss: 7.948\n",
      "predicted string : lhelll, epoch: 15, loss: 7.853\n",
      "predicted string : lhelll, epoch: 16, loss: 7.759\n",
      "predicted string : lhelll, epoch: 17, loss: 7.666\n",
      "predicted string : lhelll, epoch: 18, loss: 7.575\n",
      "predicted string : ihelll, epoch: 19, loss: 7.483\n",
      "predicted string : ihelll, epoch: 20, loss: 7.393\n",
      "predicted string : ihelll, epoch: 21, loss: 7.302\n",
      "predicted string : ihelll, epoch: 22, loss: 7.210\n",
      "predicted string : ihelll, epoch: 23, loss: 7.118\n",
      "predicted string : ihelll, epoch: 24, loss: 7.025\n",
      "predicted string : ihelll, epoch: 25, loss: 6.932\n",
      "predicted string : ihelll, epoch: 26, loss: 6.839\n",
      "predicted string : ihelll, epoch: 27, loss: 6.747\n",
      "predicted string : ihelll, epoch: 28, loss: 6.657\n",
      "predicted string : ihelll, epoch: 29, loss: 6.570\n",
      "predicted string : ihelll, epoch: 30, loss: 6.484\n",
      "predicted string : ihelll, epoch: 31, loss: 6.402\n",
      "predicted string : ihelll, epoch: 32, loss: 6.323\n",
      "predicted string : ihelll, epoch: 33, loss: 6.245\n",
      "predicted string : ihelll, epoch: 34, loss: 6.170\n",
      "predicted string : ihelll, epoch: 35, loss: 6.097\n",
      "predicted string : ihelll, epoch: 36, loss: 6.025\n",
      "predicted string : ihelll, epoch: 37, loss: 5.954\n",
      "predicted string : ihelll, epoch: 38, loss: 5.884\n",
      "predicted string : ihelll, epoch: 39, loss: 5.814\n",
      "predicted string : ihelll, epoch: 40, loss: 5.745\n",
      "predicted string : ihelll, epoch: 41, loss: 5.677\n",
      "predicted string : ihelll, epoch: 42, loss: 5.610\n",
      "predicted string : ihello, epoch: 43, loss: 5.543\n",
      "predicted string : ihello, epoch: 44, loss: 5.477\n",
      "predicted string : ihello, epoch: 45, loss: 5.410\n",
      "predicted string : ihello, epoch: 46, loss: 5.345\n",
      "predicted string : ihello, epoch: 47, loss: 5.279\n",
      "predicted string : ihello, epoch: 48, loss: 5.214\n",
      "predicted string : ihello, epoch: 49, loss: 5.149\n",
      "predicted string : ihello, epoch: 50, loss: 5.085\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "epochs = 50 \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    hidden = model.init_hidden()\n",
    "\n",
    "    print(\"predicted string : \", end='')\n",
    "    for input, label in zip(inputs, labels):\n",
    "        output, hidden = model(input, hidden)\n",
    "        val, idx = output.max(1)\n",
    "        print(idx2char[idx.item()],end='')\n",
    "        loss += criterion(output, label)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(\", epoch: %d, loss: %1.3f\" % (epoch + 1, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
