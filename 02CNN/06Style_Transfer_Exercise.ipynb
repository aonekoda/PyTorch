{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Transfer with Deep Neural Networks\n",
    "\n",
    "\n",
    "Style transfer는 content image 와 style image가 주어졌을 때 이미지의 윤곽, 형태는 content image와 유사하게, color 또는 texture 는 style image와 유사하게 바꾸는 것을 의미한다.  \n",
    "content와 style을 분리하여 다른 이미지끼리의 content와 style을 섞을 수 있다.  \n",
    "\n",
    "이 실습에서는 19-layer VGG Network를 사용하여 Style Transfer를 구현한다.  \n",
    "\n",
    " 아래 이미지에서 컨벌루션 레이어의 이름은 스택 및 스택의 순서에 따라 지정된다. Conv_1_1은 첫 번째 스택에서 이미지가 전달되는 첫 번째 컨볼루션 레이어이다. Conv_2_1은 두번째 스택에서 첫 번째 컨볼루션 레이어다. 네트워크에서 가장 깊은 컨볼루션 레이어는 conv_5_4이다.\n",
    "\n",
    "\n",
    "\n",
    "<img src='../assets/vgg19_convlayers.png' width=80% />\n",
    "\n",
    "### Separating Style and Content\n",
    "\n",
    "Style Transfer에서는 content와 style을 분리하여 다른 이미지끼리의 content와 style을 섞을 수 있다.\n",
    "\n",
    "하나의 컨텐츠 이미지와 하나의 스타일 이미지가 주어지면 원하는 컨텐츠 및 스타일 구성 요소를 조합하여 새로운 target 이미지를 작성한다.\n",
    "\n",
    "\n",
    "* **content image** : 이미지를 구성하는 주된 형태\n",
    "* **style image** : 스타일, 색깔, 텍스쳐 등\n",
    "\n",
    "content image 와 style image가 주어졌을 때 그 이미지의 주된 형태는 content image와 유사하게 유지하면서 스타일만 우리가 원하는 style image와 유사하게 바꾸는 것을 말한다. \n",
    "\n",
    "\n",
    "아래 예제에서 보듯이 고양이 사진을 content image로 하고 [Hokusai's Great Wave](https://en.wikipedia.org/wiki/The_Great_Wave_off_Kanagawa).을 style image로 하여 새로운 target image를 생성한다. 생성된 target image는 여전히 고양이의 형태를 유지하지만 색깔, texture 등은 sytle image의 것으로 대체되었다.\n",
    "\n",
    "<img src='../assets/style_tx_cat.png' width=80% />\n",
    "\n",
    "이번 실습에서 pre-trained VGG19 Net을 content, style을 추출하는 모형으로 사용한다. 그런 다음 content와 style의 losses를 사용하여 원하는 결과를 얻을 때까지 대상 이미지를 반복적으로 업데이트한다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import resources\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import requests\n",
    "from torchvision import transforms, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in VGG19 (features)\n",
    "\n",
    "VGG19 는 두 부분으로 나뉘어 있다.:\n",
    "* `vgg19.features`, 모든 convolutional layer와 pooling layers\n",
    "* `vgg19.classifier`, 맨 마지막 3개의 linear layer로 classifier layer이다.\n",
    "\n",
    "우리는 `features` 부분만 필요하다.  그리고 weight가 update되지 않도록 \"freeze\"한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the \"features\" portion of VGG19 (we will not need the \"classifier\" portion)\n",
    "import os\n",
    "\n",
    "os.environ['TORCH_HOME'] = '../models'\n",
    "\n",
    "\n",
    "vgg = models.vgg19(pretrained=True).features\n",
    "\n",
    "# freeze all VGG parameters since we're only optimizing the target image\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# move the model to GPU, if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vgg.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Content and Style Images\n",
    "\n",
    "Style transfer에 사용할  content image와 style image를 로드한다. \n",
    "`load_image` function이 해당 image를 변환하여 normalized Tensors 형태로 load한다.\n",
    "\n",
    "\n",
    "추가적으로, 사이즈가 큰 이미지를 사용하면 학습 시간이 오래 걸릴 수 있다.   \n",
    "그리고 content와 style image를 같은 사이즈로 해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_image(img_path, max_size=128, shape=None): # GPU로 학습하면 400\n",
    "    ''' Load in and transform an image, making sure the image\n",
    "       is <= 400 pixels in the x-y dims.'''\n",
    "    if \"http\" in img_path:\n",
    "        response = requests.get(img_path)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    else:\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "    \n",
    "    # large images will slow down processing\n",
    "    if max(image.size) > max_size:\n",
    "        size = max_size\n",
    "    else:\n",
    "        size = max(image.size)\n",
    "    \n",
    "    if shape is not None:\n",
    "        size = shape\n",
    "        \n",
    "    in_transform = transforms.Compose([\n",
    "                        transforms.Resize(size),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                                             (0.229, 0.224, 0.225))])\n",
    "\n",
    "    # discard the transparent, alpha channel (that's the :3) and add the batch dimension\n",
    "    image = in_transform(image)[:3,:,:].unsqueeze(0)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "content 이미지와 style 이미지를 load한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load in content and style image\n",
    "content = load_image('images/octopus.jpg').to(device)\n",
    "# Resize style to match content, makes code easier\n",
    "style = load_image('images/hockney.jpg', shape=content.shape[-2:]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper function for un-normalizing an image \n",
    "# and converting it from a Tensor image to a NumPy image for display\n",
    "def im_convert(tensor):\n",
    "    \"\"\" Display a tensor as an image. \"\"\"\n",
    "    \n",
    "    image = tensor.to(\"cpu\").clone().detach()\n",
    "    image = image.numpy().squeeze()\n",
    "    image = image.transpose(1,2,0)\n",
    "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
    "    image = image.clip(0, 1)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display the images\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "# content and style ims side-by-side\n",
    "ax1.imshow(im_convert(content))\n",
    "ax2.imshow(im_convert(style))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## VGG19 Layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print out VGG19 structure so you can see the names of various layers\n",
    "print(vgg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content and Style Features\n",
    "\n",
    "## 실습하기\n",
    "아래에서 **content representation** 및 **style representation**에 레이어 이름 매핑을 완료해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features(image, model, layers=None):\n",
    "    \"\"\" Run an image forward through a model and get the features for \n",
    "        a set of layers. Default layers are for VGGNet matching Gatys et al (2016)\n",
    "    \"\"\"\n",
    "    \n",
    "    ## TODO: Complete mapping layer names of PyTorch's VGGNet to names from the paper\n",
    "    ## Need the layers for the content and style representations of an image\n",
    "    if layers is None:\n",
    "        layers = {'0': 'conv1_1'}\n",
    "        \n",
    "        \n",
    "    ## -- do not need to change the code below this line -- ##\n",
    "    features = {}\n",
    "    x = image\n",
    "    # model._modules is a dictionary holding each module in the model\n",
    "    for name, layer in model._modules.items():\n",
    "        x = layer(x)\n",
    "        if name in layers:\n",
    "            features[layers[name]] = x\n",
    "            \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Gram Matrix \n",
    "\n",
    "## 실습하기\n",
    "Gram matrix는 기본적으로 하나의 layer에서 각 feature map 들 사이의 유사도를 측정한다.\n",
    "\n",
    "The Gram matrix는 다음과 같이 계산된다.\n",
    "\n",
    "* `batch_size, d, h, w = tensor.size` 를 사용하여 텐서의 batch_size, depth, height, width 정보를 가져온다.\n",
    "* flatten: tensor를 1차원으로 Reshape한다.\n",
    "* gram matrix는 reshape된 tensor와 자신의 transpose 을 matrix multiply 해서 구한다.\n",
    "\n",
    "**Note: 두개의 matrix는 `torch.mm(matrix1, matrix2)`로 구한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gram_matrix(tensor):\n",
    "    \"\"\" Calculate the Gram Matrix of a given tensor \n",
    "        Gram Matrix: https://en.wikipedia.org/wiki/Gramian_matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    ## get the batch_size, depth, height, and width of the Tensor\n",
    "    ## reshape it, so we're multiplying the features for each channel\n",
    "    ## calculate the gram matrix\n",
    "    gram = None\n",
    "    \n",
    "    return gram "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Putting it all Together\n",
    "\n",
    "주어진 컨볼루션 레이어의 특징을 추출하고 Gram Matrix 를 계산하는 함수를 작성했다. 이를 모두 합쳐서 이미지에서 feature를 추출하고 스타일 representaion 에서 각 레이어의 Gram Matrix 을 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get content and style features only once before forming the target image\n",
    "content_features = get_features(content, vgg)\n",
    "style_features = get_features(style, vgg)\n",
    "\n",
    "# calculate the gram matrices for each layer of our style representation\n",
    "style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n",
    "\n",
    "# create a third \"target\" image and prep it for change\n",
    "# it is a good idea to start off with the target as a copy of our *content* image\n",
    "# then iteratively change its style\n",
    "target = content.clone().requires_grad_(True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Loss and Weights\n",
    "\n",
    "#### Individual Layer Style Weights\n",
    "\n",
    "각 관련 레이어에서 스타일 표현에 가중치를 부여하는 옵션을 줄 수 있다. 레이어의 가중치는 0에서 1 사이의 범위를 사용하는 것이 좋다.  `conv1_1` 및`conv2_1`에 가중치를 더 많이 부여하면 최종 target 이미지에서 더 많은 style의 artifacts를 반영할 수 있다. \n",
    "\n",
    "#### Content and Style Weight\n",
    "\n",
    "alpha (`content_weight`)와 beta (`style_weight`)를 정의한다. 이 비율은 최종 이미지의 스타일에 영향을 준다. content_weight = 1을 그대로 두고 원하는 비율을 달성하도록 style_weight를 설정하는 것이 좋다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# weights for each style layer \n",
    "# weighting earlier layers more will result in *larger* style artifacts\n",
    "# notice we are excluding `conv4_2` our content representation\n",
    "style_weights = {'conv1_1': 1.,\n",
    "                 'conv2_1': 0.8,\n",
    "                 'conv3_1': 0.5,\n",
    "                 'conv4_1': 0.3,\n",
    "                 'conv5_1': 0.1}\n",
    "\n",
    "# you may choose to leave these as is\n",
    "content_weight = 1  # alpha\n",
    "style_weight = 1e6  # beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating the Target & Calculating Losses\n",
    "\n",
    "## 실습하기\n",
    "\n",
    "이미지를 업데이트하기위한 여러 단계를 설정해야 한다.  \n",
    "target_ 이미지 만 변경하고 VGG19 또는 다른 이미지는 변경하지 않는다. \n",
    "좋은 결과를 얻으려면 최소 2000 steps 를 사용하는 것이 좋다. 그러나 처음에는 훨씬 적은 step 수 부터 실행해야 훈련에 걸리는 시간을 줄일 수 있다.  \n",
    "\n",
    "반복 루프 내에서 컨텐츠 및 스타일 손실을 계산하고 이에 따라 target 이미지를 업데이트한다.\n",
    "\n",
    "#### Content Loss\n",
    "\n",
    "콘텐츠 loss는 'conv4_2'레이어에서 target과 content feature 간의 MSE로 구한다.\n",
    "```\n",
    "content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2'])**2)\n",
    "```\n",
    "\n",
    "#### Style Loss\n",
    "\n",
    "style loss 는 target 이미지와 Style 이미지 사이의 loss이다. 즉, style image의 gram matrix와 target image의 gram matrix 의 차이를 말한다. Loss는 MSE를 사용하여 계산한다.\n",
    "\n",
    "#### Total Loss\n",
    "\n",
    "마지막으로 스타일과 콘텐츠 손실을 합산하고 지정된 알파 및 베타로 가중치를 적용하여 총 loss를 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for displaying the target image, intermittently\n",
    "show_every = 400\n",
    "\n",
    "# iteration hyperparameters\n",
    "optimizer = optim.Adam([target], lr=0.003)\n",
    "steps = 2000  # decide how many iterations to update your image (5000)\n",
    "\n",
    "for ii in range(1, steps+1):\n",
    "    \n",
    "    ## TODO: get the features from your target image    \n",
    "    ## Then calculate the content loss\n",
    "    target_features = None\n",
    "    content_loss = None\n",
    "    \n",
    "    # the style loss\n",
    "    # initialize the style loss to 0\n",
    "    style_loss = 0\n",
    "    # iterate through each style layer and add to the style loss\n",
    "    for layer in style_weights:\n",
    "        # get the \"target\" style representation for the layer\n",
    "        target_feature = target_features[layer]\n",
    "        _, d, h, w = target_feature.shape\n",
    "        \n",
    "        ## TODO: Calculate the target gram matrix\n",
    "        target_gram = None\n",
    "        \n",
    "        ## TODO:  get the \"style\" style representation\n",
    "        style_gram = None\n",
    "        ## TODO: Calculate the style loss for one layer, weighted appropriately\n",
    "        layer_style_loss = None\n",
    "        \n",
    "        # add to the style loss\n",
    "        style_loss += layer_style_loss / (d * h * w)\n",
    "        \n",
    "        \n",
    "    ## TODO:  calculate the *total* loss\n",
    "    total_loss = None\n",
    "    \n",
    "    ## -- do not need to change code, below -- ##\n",
    "    # update your target image\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # display intermediate images and print the loss\n",
    "    if  ii % show_every == 0:\n",
    "        print('Total loss: ', total_loss.item())\n",
    "        plt.imshow(im_convert(target))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the Target Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display content and final, target image\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax1.imshow(im_convert(content))\n",
    "ax2.imshow(im_convert(target))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
