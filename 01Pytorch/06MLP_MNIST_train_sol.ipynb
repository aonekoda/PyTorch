{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Networks\n",
    "\n",
    "Neural Network를 정의하고 훈련하여 MNIST 데이터 셋으로부터 손글씨 숫자를 인식하도록 하자.\n",
    "\n",
    "![img](../assets/function_approx.png)\n",
    "\n",
    "훈련 데이터로 신경망의 parameter (=weights,bias)를 우리가 원하는 답에 가깝게 근사하도록 해야 한다.\n",
    "\n",
    "이러한 parameters를 찾기 위해서,**loss function** (=cost)를 사용한다. 회귀문제에서 loss 함수는 mean squared loss를 사용하고 분류 문제를 위해서는 cross entropy를 주로 활용한다.\n",
    "\n",
    "신경망의 학습 목표는 이러한 loss를 최소화하는 parameter를 찾는 것에 있다. 이러한 과정은  **gradient descent**알고리즘을 활용한다. gradient 는 어떤 한점에서 loss function의 기울기를 말하고 이것은 해당 loss를 가장 빠르게 감소할수 있는 방향과 크기를 나타낸다. loss를 최소화하는 방향으로 진행하는 학습은 높은 산을 내려가는 방법과 비교할 수 있다.\n",
    "\n",
    "![img](../assets/gradient_descent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "깊이가 깊은 multilayer neural network에서는 gradient를 계산하기 쉽지 않다. multilayer networks를 훈련하기 위한 다양한 연구가 진행되어 왔다.\n",
    "\n",
    "multilayer networks를 훈련하기 위한 방법이 **backpropagation** 이다. 궁극적으로 backpropagation은 합성함수 미분에서 chain rule를 구현하는 것이다. 이해하기 쉽도록 2 layer network의 computational graph로 표현하면 다음과 같다.\n",
    "![img](../assets/backprop_diagram.png)\n",
    "\n",
    "weights를 학습하기 위해 gradient descent를 사용한다. backpropagation은 loss를 backwards로 전파한다.\n",
    "\n",
    "$$\n",
    "\\large \\frac{\\partial \\ell}{\\partial W_1} = \\frac{\\partial L_1}{\\partial W_1} \\frac{\\partial S}{\\partial L_1} \\frac{\\partial L_2}{\\partial S} \\frac{\\partial \\ell}{\\partial L_2}\n",
    "$$\n",
    "\n",
    "weights의 update 는 다음과 같이 수행된다. 이 때 $\\alpha$는 learning rate이다.  \n",
    "\n",
    "$$\n",
    "\\large W^\\prime_1 = W_1 - \\alpha \\frac{\\partial \\ell}{\\partial W_1}\n",
    "$$\n",
    "\n",
    "learning rate $\\alpha$ 는 weight update steps으로 update 속도와 관련되어 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses in PyTorch\n",
    "\n",
    "`nn` module은 cross-entropy loss (`nn.CrossEntropyLoss`)와 같은 loss함수를 제공한다. 보통 `criterion`이라는 이름으로 할당한다. MNIST와 같은 분류 문제에서 출력층의 활성화 함수는 softmax function를 사용하여 각 class별 확률을 출력한다. loss함수로는 cross-entropy를 사용한다. \n",
    "\n",
    "이때 주의해야 할 점이 있다. [the documentation for `nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss),\n",
    "\n",
    "> `nn.LogSoftmax()` 와 `nn.NLLLoss()` 를 하나로 결합한 것이 cross-entropy이다.\n",
    ">\n",
    "> 신경망의 최종 출력은 각 class에 해당하는 score이다.\n",
    "\n",
    "즉, 신경망의 최종 출력은 softmax 활성화 함수 없이 그대로 score만을 출력하고, cross-entropy를 loss함수로 적용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5, )),\n",
    "                              ])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('../data', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2857, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10))\n",
    "\n",
    "# Define the loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Get our data\n",
    "images, labels = next(iter(trainloader))\n",
    "# Flatten images\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# Forward pass, get our logits\n",
    "logits = model(images)\n",
    "# Calculate the loss with the logits and the labels\n",
    "loss = criterion(logits, labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력층의 활성화 함수로 log-softmax `nn.LogSoftmax` 또는 `F.log_softmax` ([documentation](https://pytorch.org/docs/stable/nn.html#torch.nn.LogSoftmax))를 적용하는 형태로 구성 가능하다. 그렇게 되면 출력되는 output에 exponential `torch.exp(output)`을 적용하여 class의 확률을 구할 수 있다. log-softmax output에 negative log likelihood loss, `nn.NLLLoss` ([documentation](https://pytorch.org/docs/stable/nn.html#torch.nn.NLLLoss))를 loss함수로 적용한다.\n",
    "\n",
    ">**실습 :** log-softmax를 출력층의 활성화 함수로 지정하시오.  \n",
    "loss 함수는 negative log likelihood loss로 지정하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2925, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "## Solution\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "# Define the loss\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Get our data\n",
    "images, labels = next(iter(trainloader))\n",
    "# Flatten images\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# Forward pass, get our log-probabilities\n",
    "logps = model(images)\n",
    "# Calculate the loss with the logps and the labels\n",
    "loss = criterion(logps, labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "\n",
    "Torch 는 `autograd` module을 제공한다. `autograd`는 자동으로 텐서의 gradients 를 계산한다. `autograd`를 사용하여 parameters의 gradient를 구한다.  텐서를 생성할 때 `requires_grad = True` 를 쓴다. 또는  `x.requires_grad_(True)`를 사용한다.\n",
    "\n",
    "gradient의 자동 계산 기능을 비활성화 하기 위해서는 `torch.no_grad()` 를 사용한다.:\n",
    "```python\n",
    "x = torch.zeros(1, requires_grad=True)\n",
    ">>> with torch.no_grad():\n",
    "...     y = x * 2\n",
    ">>> y.requires_grad\n",
    "False\n",
    "```\n",
    "\n",
    "`torch.set_grad_enabled(True|False)`를 사용하여 활성화/비활성화 할 수 있다.\n",
    "\n",
    "gradient를 계산하기 위해서는 backward() 메소드를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4925,  0.9029],\n",
      "        [-1.7136,  0.3031]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2,2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2425, 0.8153],\n",
      "        [2.9363, 0.0919]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x**2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y`를 생성하는데 사용된 함수를 확인할 수 있다. `PowBackward0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PowBackward0 object at 0x00000266F46392E8>\n"
     ]
    }
   ],
   "source": [
    "## grad_fn shows the function that generated this variable\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autograd module 은 텐서에 수행된 operations을 추적하고 각각에 대한 gradient를 자동으로 계산할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0215, grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "z = y.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient를 계산하기 전에는  `x` 와 `y` gradient는 비어있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " `.backward` method로 gradient를 계산한다. \n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial}{\\partial x}\\left[\\frac{1}{n}\\sum_i^n x_i^2\\right] = \\frac{x}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2462,  0.4515],\n",
      "        [-0.8568,  0.1515]])\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Autograd together\n",
    "\n",
    "PyTorch에서 신경망을 구현하면, 기본적으로 모든 parameters가 `requires_grad = True`로 초기화된다. 따라서 gradient를 계산하기 위해서는 `loss.backward()`만 호출해주면 된다. 이 gradient 값은 weights를 update하는데 사용된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logps = model(images)\n",
    "loss = criterion(logps, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before backward pass: \n",
      " None\n",
      "After backward pass: \n",
      " tensor([[-0.0005, -0.0005, -0.0005,  ..., -0.0005, -0.0005, -0.0005],\n",
      "        [ 0.0003,  0.0003,  0.0003,  ...,  0.0003,  0.0003,  0.0003],\n",
      "        [ 0.0085,  0.0085,  0.0085,  ...,  0.0085,  0.0085,  0.0085],\n",
      "        ...,\n",
      "        [ 0.0001,  0.0001,  0.0001,  ...,  0.0001,  0.0001,  0.0001],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0025, -0.0025, -0.0025,  ..., -0.0025, -0.0025, -0.0025]])\n"
     ]
    }
   ],
   "source": [
    "print('Before backward pass: \\n', model[0].weight.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('After backward pass: \\n', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network!\n",
    "\n",
    "optimizer를 설정한다. optimizer는 앞서 구한 gradient값을 적용해서 parameter를 update한다. PyTorch의 [`optim` package](https://pytorch.org/docs/stable/optim.html)을 import한다. 예를 들어 stochastic gradient descent는 `optim.SGD`로 구현된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# Optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 data를 훈련하기에 앞서 loop를 1회만 수행해 보도록 하자.  \n",
    "\n",
    "일반적으로 PyTorch에서는 다음과 같은 절차로 수행한다.:\n",
    "\n",
    "* Make a forward pass through the network \n",
    "* Use the network output to calculate the loss\n",
    "* Perform a backward pass through the network with `loss.backward()` to calculate the gradients\n",
    "* Take a step with the optimizer to update the weights\n",
    "\n",
    "각 단계에 대해 weight값을 출력해 본다. 이때 주의할 것은 `optimizer.zero_grad()` 부분이다. gradients는 호출될때 누적되어 계산된다. 따라서 각 loop에서 `optimizer.zero_grad()`를 호출하여 0으로 초기화 해 주어야만 정확한 gradient가 계산된다. 즉 각 batch별로 gradient를 loop 돌때 0으로 초기화 하라는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights -  Parameter containing:\n",
      "tensor([[-3.2170e-02, -2.9442e-02,  2.7169e-02,  ..., -1.1596e-02,\n",
      "         -7.5351e-03, -1.2622e-02],\n",
      "        [-1.2739e-02, -2.5327e-02, -2.0190e-02,  ..., -3.4283e-02,\n",
      "          3.3336e-02,  1.3475e-02],\n",
      "        [ 3.4116e-02,  1.8554e-02,  3.0802e-02,  ..., -2.6830e-02,\n",
      "          3.1995e-02, -6.6940e-05],\n",
      "        ...,\n",
      "        [ 1.0757e-02, -3.1427e-02, -3.0058e-02,  ..., -2.6827e-02,\n",
      "          3.1666e-02, -2.6292e-02],\n",
      "        [ 8.5137e-03, -1.4527e-02,  2.2820e-02,  ...,  6.2967e-03,\n",
      "          2.1455e-02,  2.7212e-02],\n",
      "        [ 1.2841e-02, -3.1315e-02,  1.4111e-02,  ...,  1.1578e-02,\n",
      "          1.4227e-02,  1.7372e-03]], requires_grad=True)\n",
      "Gradient - tensor([[ 2.7235e-03,  2.7235e-03,  2.7235e-03,  ...,  2.7235e-03,\n",
      "          2.7235e-03,  2.7235e-03],\n",
      "        [ 1.5184e-03,  1.5184e-03,  1.5184e-03,  ...,  1.5184e-03,\n",
      "          1.5184e-03,  1.5184e-03],\n",
      "        [ 1.4317e-03,  1.4317e-03,  1.4317e-03,  ...,  1.4317e-03,\n",
      "          1.4317e-03,  1.4317e-03],\n",
      "        ...,\n",
      "        [-6.8841e-05, -6.8841e-05, -6.8841e-05,  ..., -6.8841e-05,\n",
      "         -6.8841e-05, -6.8841e-05],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-2.2801e-03, -2.2801e-03, -2.2801e-03,  ..., -2.2801e-03,\n",
      "         -2.2801e-03, -2.2801e-03]])\n"
     ]
    }
   ],
   "source": [
    "print('Initial weights - ', model[0].weight)\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(64, 784)\n",
    "\n",
    "# Clear the gradients, do this because gradients are accumulated\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass, then backward pass, then update weights\n",
    "output = model(images)\n",
    "loss = criterion(output, labels)\n",
    "loss.backward()\n",
    "print('Gradient -', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights -  Parameter containing:\n",
      "tensor([[-3.2197e-02, -2.9469e-02,  2.7142e-02,  ..., -1.1623e-02,\n",
      "         -7.5623e-03, -1.2649e-02],\n",
      "        [-1.2754e-02, -2.5342e-02, -2.0205e-02,  ..., -3.4298e-02,\n",
      "          3.3321e-02,  1.3460e-02],\n",
      "        [ 3.4102e-02,  1.8539e-02,  3.0788e-02,  ..., -2.6844e-02,\n",
      "          3.1981e-02, -8.1257e-05],\n",
      "        ...,\n",
      "        [ 1.0758e-02, -3.1426e-02, -3.0057e-02,  ..., -2.6826e-02,\n",
      "          3.1666e-02, -2.6291e-02],\n",
      "        [ 8.5137e-03, -1.4527e-02,  2.2820e-02,  ...,  6.2967e-03,\n",
      "          2.1455e-02,  2.7212e-02],\n",
      "        [ 1.2864e-02, -3.1292e-02,  1.4133e-02,  ...,  1.1601e-02,\n",
      "          1.4249e-02,  1.7600e-03]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Take an update step and few the new weights\n",
    "optimizer.step()\n",
    "print('Updated weights - ', model[0].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training for real\n",
    "\n",
    "실제로 test 데이터 셋을 가지고 전체 훈련과정을 구현해 보자. 이때 *epoch*은 전체 훈련 데이터를 처리하는 반복 횟수이다. `trainloader`에서 batches 단위로 데이터를 가져와서 각 batch별로 훈련을 진행한다.각 training pass별로 loss를 계산하고, backwards pass를 수행해서, weights를 update한다.\n",
    "\n",
    "> **실습 :** training pass를 구현하시오. 제대로 훈련이 진행된다면 각 epoch를 돌면서 loss가 줄어드는 것을 확인할 수 있다. (5개 epoch을 수행하는데 약 3분~5분정도 소요될 수 있다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 51.49688295311511\n",
      "Training loss: 16.81396443020306\n",
      "Training loss: 12.717651508764417\n",
      "Training loss: 10.8232765673066\n",
      "Training loss: 9.75078184238629\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        # TODO: Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        \n",
    "        loss = criterion(output, labels)*images.size(0)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망의 훈련이 끝나면 제대로 분류하는지 테스트를 수행해 본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def view_classify(img, ps):\n",
    "    ''' Function for viewing an image and it's predicted classes.\n",
    "    '''\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    \n",
    "    ax2.set_yticklabels(np.arange(10))\n",
    "    \n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logps = model(img)\n",
    "\n",
    "# Output of the network are log-probabilities, need to take exponential for probabilities\n",
    "ps = torch.exp(logps)\n",
    "view_classify(img.view(1, 28, 28), ps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
